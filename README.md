# SightScribe

SightScribe is an AI companion designed to make lectures and presentations accessible â€” especially for people with visual impairments. Point your phone at the screen and SightScribe uses computer vision (CLIP image embeddings plus OCR text matching) to detect slide changes, interprets the on-screen content with Gemini, and turns it into spoken explanations in real time.

Learners ask questions about the current slideâ€”or about earlier slides they couldnâ€™t seeâ€”using ElevenLabs speech-to-text. SightScribe routes the transcribed query with the right context to a lightweight LLM for fast reasoning, then returns the answer through ElevenLabs text-to-speech so the conversation never disrupts the presenter.

Instead of passively listening, users interact with the material as it happens: asking follow-ups, revisiting previous slides, and hearing details that would otherwise be missed. See through sound. Learn without barriers.

## Environment Setup

Install the tooling you need to work locally:

- Flutter SDK 3.9.2 or newer (includes Dart)
- Xcode (macOS) or Android SDK + Android Studio/VS Code
- Python 3.11+ for the FastAPI backend (a `.venv` is recommended)
- Node.js 18+ if you plan to run the optional signaling server for remote camera sharing

Clone the repository and install Flutter dependencies:

```bash
git clone https://github.com/SuyashManiyar/HackUMass_codebase.git
cd HackUMass_codebase
flutter pub get
```

Create `.env` beside `lib/main.dart` and provide the runtime keys:

```
FASTAPI_BASE_URL=http://127.0.0.1:8000
SIGNALING_SERVER_URL=http://127.0.0.1:3000
OPENROUTER_API_KEY=sk-...
ELEVENLABS_API_KEY=...
GEMINI_API_KEY=...
SLIDE_CAPTURE_INTERVAL=10
```

The `.env` powers both camera capture (FastAPI), question answering (OpenRouter), and audio interfaces (ElevenLabs). The backend also honors `GEMINI_API_KEY` for slide summarization.

## Before You Run

1. **Bootstrap the Python backend**
   ```bash
   cd HackUMass_codebase
   python -m venv .venv
   source .venv/bin/activate
   pip install -r server/requirements.txt
   export GEMINI_API_KEY=...
   uvicorn server.main:app --host 0.0.0.0 --port 8000
   ```
2. **(Optional) Start the signaling server** for remote camera sharing:
   ```bash
   cd signaling-server
   npm install
   npm start
   ```
3. **Launch Flutter** on your device or simulator:
   ```bash
   flutter run
   ```

Once the backend is up and the Flutter app is running, tap **Start Capturing** in the app to begin a session.

## GuideLens Pipeline

#### âœ… Full System Pipeline (Condensed w/ Arrows)

```
START SESSION
     â†“
Camera video feed â†’ Flutter client
     â†“ (periodic frames)
Send frame â†’ FastAPI /process_slide
     â†“
[FastAPI]
  â€¢ CLIP similarity vs last slide
  â€¢ OCR â†’ text similarity
     â†“
New slide decision rule:
  IF (CLIP < 0.88) OR (TextSim < 0.65) â†’ NEW SLIDE
  ELSE â†’ SAME SLIDE â†’ return latest summary
     â†“
IF NEW SLIDE:
     â†“
Gemini 2.5 Flash â†’ structured JSON:
  {
    title[], enumeration[], equations[],
    display_summary[], end_summary[], ... etc.
  }
     â†“
Store summary â†’ update:
  â€¢ latest slide
  â€¢ slide history[]
     â†“
Return JSON â†’ Flutter
     â†“
UI shows â†’ â€œSlide Summaryâ€
```

#### âœ… Voice Pipeline

```
User taps mic â†’ record speech
     â†“
Speech-to-Text (ElevenLabs / STT)
     â†“
Text query + latest slide summary
+ slide history (context)
     â†“
Send â†’ OpenRouter (GPT-4o-mini)
     â†“
LLM generates short 25-word reply
     â†“
Text-To-Speech â†’ ElevenLabs TTS
     â†“
User hears answer
     â†“
Follow-up questions allowed (loop)
```

#### âœ… Session Finalization

```
At Stop Capturing:
     â†“
We have:
  â€¢ slide_history[]
  â€¢ per-slide end_summary[]
     â†“
Generate overall lecture summary (Gemini)
     â†“
Store:
  â€¢ all slide JSONs
  â€¢ overall_summary
     â†“
Show â€œEnd Summaryâ€ screen
```

#### ðŸ”¹ Components

**Client (Flutter)** â€“ captures frames, renders summaries, manages slide history, and handles the voice assistant UX.  
**Backend (FastAPI)** â€“ performs frame differencing (CLIP + OCR), calls Gemini, maintains session state, and returns structured JSON.  
**Models** â€“ CLIP for similarity, OCR for text extraction, Gemini for slide summaries, OpenRouter for QA, and ElevenLabs for both STT and TTS.

#### âœ… Key Logic Rules

- Slide is replaced only if:

  ```
  clip_sim < 0.88   OR
  text_sim < 0.65
  ```

- Only a **new** slide triggers Gemini extraction and history append.

## Project Structure

```
lib/
â”œâ”€â”€ core/                       # App state + env helpers
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ camera/                 # Camera controller & capture service
â”‚   â”œâ”€â”€ slide_pipeline/         # Slide client, repository, scheduler
â”‚   â”œâ”€â”€ voice_pipeline/         # Conversation controller + STT/TTS
â”‚   â”œâ”€â”€ llm/                    # OpenRouter bridge
â”‚   â””â”€â”€ screens/                # Supporting UI flows
â”œâ”€â”€ services/                   # FastAPI client, signaling, etc.
â”œâ”€â”€ utils/                      # Logger, debouncer, helpers
â””â”€â”€ main.dart                   # App entry, primary UI

server/
â”œâ”€â”€ main.py                     # FastAPI entry point
â”œâ”€â”€ routers/                    # /process_slide, /health
â””â”€â”€ core/                       # CLIP, OCR, Gemini orchestration

signaling-server/
â””â”€â”€ index.js                    # Optional WebRTC signaling server
```

## Troubleshooting

- Run `flutter clean && flutter pub get` if builds fail.
- Ensure `.env` is present and populated before launching the app.
- If the backend rejects requests, confirm `GEMINI_API_KEY`, `OPENROUTER_API_KEY`, and `ELEVENLABS_API_KEY` are exported.
- To reset the voice assistant, stop capturing and start again; this reinitializes the conversation controller.

## Contributing

Pull requests are welcomeâ€”keep code formatted with `flutter format .` and ensure both backend and Flutter app run locally (`uvicorn` and `flutter run`) before submitting.

## Project Structure

```
lib/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ app_state.dart              # Slide summary state
â”‚   â””â”€â”€ env.dart                    # Environment helpers (.env access)
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ camera/
â”‚   â”‚   â”œâ”€â”€ camera_capture_service.dart
â”‚   â”‚   â””â”€â”€ camera_controller.dart
â”‚   â”œâ”€â”€ slide_pipeline/
â”‚   â”‚   â”œâ”€â”€ slide_client.dart       # POST /process_slide
â”‚   â”‚   â”œâ”€â”€ slide_repo.dart         # Stores latest OCR + summary
â”‚   â”‚   â””â”€â”€ slide_scheduler.dart    # Periodic capture loop
â”‚   â”œâ”€â”€ voice_pipeline/
â”‚   â”‚   â”œâ”€â”€ conversation_controller.dart
â”‚   â”‚   â”œâ”€â”€ voice_pipeline.dart
â”‚   â”‚   â”œâ”€â”€ stt/
â”‚   â”‚   â””â”€â”€ tts/
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ llm_service.dart
â”‚   â””â”€â”€ screens/
â”‚       â”œâ”€â”€ connect_camera_screen.dart
â”‚       â”œâ”€â”€ remote_camera_view_screen.dart
â”‚       â”œâ”€â”€ share_camera_screen.dart
â”‚       â””â”€â”€ test_pipeline_page.dart
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ camera_stream_service.dart
â”‚   â”œâ”€â”€ fastapi_client.dart        # Shared FastAPI HTTP client
â”‚   â”œâ”€â”€ remote_camera_manager.dart
â”‚   â””â”€â”€ signaling_service.dart
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ connection_monitor.dart
â”‚   â”œâ”€â”€ debouncer.dart
â”‚   â”œâ”€â”€ error_handler.dart
â”‚   â””â”€â”€ logger.dart
â””â”€â”€ main.dart                      # App entry point & home UI

server/
â”œâ”€â”€ main.py                        # FastAPI application
â”œâ”€â”€ routers/                       # /process_slide & /health
â””â”€â”€ core/                          # OCR, Gemini, change detection

signaling-server/
â”œâ”€â”€ index.js
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

## Troubleshooting

### Flutter Issues

**"flutter: command not found"**
- Install Flutter SDK and add to PATH
- Verify with: `flutter doctor`

**Build errors**
```bash
flutter clean
flutter pub get
flutter run
```

**Permission errors**
- Check AndroidManifest.xml has camera/microphone permissions
- Check Info.plist has usage descriptions (iOS)

### Signaling Server Issues

**Cannot connect to server**
- Ensure server is running: `npm start` in signaling-server directory
- Check firewall allows connections on port 3000
- Verify server URL in app matches your server's IP

**"Port already in use"**
```bash
PORT=3001 npm start
```
Then update the app's server URL accordingly

### Connection Issues

**"Invalid or expired pairing code"**
- Codes expire after 1 hour
- Codes are single-use
- Ensure sender started sharing before receiver connects

**"Connection timeout"**
- Check both devices are on the same network or have internet access
- Verify signaling server is accessible from both devices
- Check firewall settings

**Poor video quality**
- Check network connection strength
- Move closer to WiFi router
- Close other bandwidth-intensive apps

## Network Requirements

- **Local Network Testing**: Both devices on same WiFi
- **Internet Testing**: Both devices need internet access
- **Bandwidth**: ~1-2 Mbps for 720p video
- **Latency**: Best with < 100ms ping

## Security Notes

- Video data is encrypted using WebRTC's built-in DTLS-SRTP
- Pairing codes expire after 1 hour
- Signaling server doesn't store or route video data
- All video streams are peer-to-peer

## Known Limitations

- Frame capture from remote stream requires platform-specific implementation
- Connection quality monitoring is basic (can be enhanced)
- No support for multiple simultaneous connections
- Pairing codes are case-insensitive but displayed in uppercase

## Future Enhancements

- [ ] Implement frame capture from remote stream
- [ ] Add support for multiple receivers
- [ ] Enhanced connection quality monitoring
- [ ] Recording capability
- [ ] Chat/messaging during video call
- [ ] Screen sharing option

## Dependencies

### Flutter Packages
- `flutter_webrtc`: ^0.9.48 - WebRTC implementation
- `socket_io_client`: ^2.0.3+1 - WebSocket client
- `camera`: ^0.10.5+9 - Camera access
- `permission_handler`: ^11.3.0 - Runtime permissions
- `image_picker`: ^1.0.7 - Image selection
- `google_generative_ai`: ^0.4.0 - Gemini AI
- `path_provider`: ^2.1.2 - File system paths

### Node.js Packages
- `express`: ^4.18.2 - Web server
- `socket.io`: ^4.6.1 - WebSocket server
- `cors`: ^2.8.5 - CORS middleware

## License

MIT

## Support

For issues and questions:
1. Check the troubleshooting section
2. Review signaling server logs
3. Check Flutter console output
4. Ensure all prerequisites are installed

## Contributing

Contributions are welcome! Please ensure:
- Code follows Flutter best practices
- All features are tested on both Android and iOS
- Documentation is updated
